{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir(\"E:\\project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "loan_data = pd.read_csv(\"bank-loan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tells unique value count in each variable\n",
    "for feature in loan_data.columns:\n",
    "    print(feature ,':', loan_data[feature].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to required data types\n",
    "loan_data['ed'] = loan_data['ed'].astype('category')\n",
    "loan_data['default'] = loan_data['default'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the train and test data-set.\n",
    "# here train set will be used to bulid and test the Model\n",
    "# The Model will be used to predict the value of target column in the test set\n",
    "train = pd.DataFrame(loan_data.loc[pd.notnull(loan_data['default'])])\n",
    "test = pd.DataFrame(loan_data.loc[pd.isnull(loan_data['default'])])\n",
    "# removing default variable from test\n",
    "test = test.drop([test.columns[-1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Numerical and categorical variables in different list\n",
    "num_var = train.select_dtypes([np.number]).columns\n",
    "cat_var = train.select_dtypes(['category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# taking a look at the data\n",
    "# gives first five rows of data set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of variables and obervation in the train data-set\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking types\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting further info about the train data-set\n",
    "# data type of data\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting summary of the train data-set\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of variables and obervation in the test data-set\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking types\n",
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the info about the test data-set\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting summary of test data-set\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values in a column\n",
    "train[train.columns[-1]].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of unique values in a column\n",
    "train[train.columns[-1]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.columns[-1]].value_counts()\n",
    "# we have 517 '0' values and 183 '1' values\n",
    "# we can see that the data is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting grid for all charts\n",
    "sns.set_style('darkgrid')\n",
    "# visualizing target variable\n",
    "plt.figure(figsize=(5,5))\n",
    "train[train.columns[-1]].value_counts().plot.pie(autopct='%1.2f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variable of train data set\n",
    "plt.figure(figsize=(14,5))\n",
    "for i,col in enumerate(cat_var):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    sns.countplot(train[col], alpha=0.6)  \n",
    "    plt.xlabel(col,fontsize=20)\n",
    "    plt.ylabel('count',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variable of test data set\n",
    "plt.figure(figsize=(14,5))\n",
    "for i,col in enumerate(cat_var[0:-1]):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    sns.countplot(test[col], alpha=0.6)\n",
    "    plt.xlabel(col,fontsize=20)\n",
    "    plt.ylabel('count',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "g = sns.PairGrid(train[train.select_dtypes([np.number]).columns])\n",
    "g.map_diag(plt.hist, color='orange', alpha=0.8)\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_lower(sns.lineplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw histograms of numeric data in training set \n",
    "print(\"Distribution of Train features\")\n",
    "plt.figure(figsize=(30,40))\n",
    "for i,col in enumerate(num_var):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    #plt.hist(train[col], bins='auto')\n",
    "    sns.distplot(train[col], kde=False)\n",
    "    plt.xlabel(col,fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw histograms of numeric data in Test set \n",
    "print(\"Distribution of Test features\")\n",
    "plt.figure(figsize=(30,40))\n",
    "for i,col in enumerate(num_var):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    #plt.hist(train[col], bins='auto', color='orange')\n",
    "    sns.distplot(test[col], kde=False, color='orange')\n",
    "    plt.xlabel(col,fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of features per target class\n",
    "# Kerner Density Estimate (KDE)\n",
    "print(\"Distribution of features per target class\")\n",
    "plt.figure(figsize=(30,40))\n",
    "for i,col in enumerate(num_var):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    sns.distplot(train[train[train.columns[-1]]==0][col],hist=False,label='0',color='blue')\n",
    "    sns.distplot(train[train[train.columns[-1]]==1][col],hist=False,label='1',color='orange')\n",
    "    plt.xlabel(col,fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of features for test and train dataset\n",
    "print(\"Distribution of features for test and train dataset\")\n",
    "plt.figure(figsize=(30,40))\n",
    "for i,col in enumerate(num_var):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    sns.distplot(train[col],hist=False,label='train',color='blue')\n",
    "    sns.distplot(test[col],hist=False,label='test',color='orange')\n",
    "    plt.xlabel(col,fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw boxplot of numeric data in Train set \n",
    "print(\"Boxplot of Train features\")\n",
    "plt.figure(figsize=(30,40))\n",
    "for i,col in enumerate(train.columns.values[0:-1]):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    sns.boxplot(x=train.columns[-1], y=col, data=train, orient='v')\n",
    "    plt.xlabel(train.columns[-1],fontsize=30)\n",
    "    plt.ylabel(col,fontsize=30)\n",
    "# from plots we can clearly see that there are outliers in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting count of missing values in train\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting count of missing values in test\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # handling outliers in Train data-set\n",
    "# Q1 = train.quantile(0.25)\n",
    "# Q3 = train.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# for feature in num_var:\n",
    "#     train.loc[(train[feature] < (Q1[feature] - 1.5 * IQR[feature])) | \n",
    "#               (train[feature] > (Q3[feature] + 1.5 * IQR[feature])),feature] = np.nan\n",
    "    \n",
    "#     #Impute with mean\n",
    "#     #train[column_name] = train[column_name].fillna(train[column_name].mean())\n",
    "\n",
    "#     #Impute with median\n",
    "#     train[feature] = train[feature].fillna(train[feature].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # handling outliers in Test data-set\n",
    "# Q1 = test.quantile(0.25)\n",
    "# Q3 = test.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# for feature in num_var:\n",
    "#     test.loc[(test[feature] < (Q1[feature] - 1.5 * IQR[feature])) | \n",
    "#               (test[feature] > (Q3[feature] + 1.5 * IQR[feature])),feature] = np.nan\n",
    "    \n",
    "#     #Impute with mean\n",
    "#     #test[feature] = test[feature].fillna(test[feature].mean())\n",
    "\n",
    "#     #Impute with median\n",
    "#     test[feature] = test[feature].fillna(test[feature].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "corr = train.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax = sns.heatmap(corr, mask=mask, cmap=sns.diverging_palette(200, 20, as_cmap=True), square=True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(corr.values,np.nan)\n",
    "corr.max().max(),corr.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chisquare test of independence\n",
    "from scipy.stats import chi2_contingency\n",
    "# loop for chi square values\n",
    "for i in cat_var:\n",
    "    print(i)\n",
    "    chi2, p, dof, ex = chi2_contingency(pd.crosstab(train[cat_var[-1]], train[i]))\n",
    "    print(p)\n",
    "# replace variables whose p-value is more than 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummyvariables for categorical data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "train = pd.DataFrame(ct.fit_transform(train))\n",
    "test = pd.DataFrame(ct.fit_transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.insert(4, 'col', 0)\n",
    "test.columns = range(test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.astype(np.integer)\n",
    "test = test.astype(np.integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dependent and independent variable in different array for model training\n",
    "X = train.iloc[:, 0:-1].values\n",
    "y = train.iloc[:, -1].values\n",
    "test_result = test.iloc[:, 0:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method of scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "test_result = sc.transform(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def classification_model(model):\n",
    "    \n",
    "    select_model = model\n",
    "    \n",
    "    if select_model=='logistic':\n",
    "        # Fitting Logistic Regression to the Training set\n",
    "        classifier = LogisticRegression(random_state = 42).fit(X_train, y_train)\n",
    "        \n",
    "    elif select_model=='KNN':\n",
    "        # Fitting K-NN to the Training set\n",
    "        classifier = KNeighborsClassifier(n_neighbors = 7, metric = 'minkowski', p = 2).fit(X_train, y_train)\n",
    "    \n",
    "    elif select_model=='NB':\n",
    "        # Fitting Naive Bayes to the Training set\n",
    "        classifier = GaussianNB().fit(X_train, y_train)\n",
    "        \n",
    "    elif select_model=='DT':\n",
    "        # Fitting Decision Tree Classification to the Training set    \n",
    "        classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 42).fit(X_train, y_train)\n",
    "        \n",
    "    elif select_model=='RF':\n",
    "        # Fitting Random Forest Classification to the Training set\n",
    "        classifier = RandomForestClassifier(n_estimators = 625,\n",
    "                                               criterion = 'entropy', random_state = 42).fit(X_train, y_train)\n",
    "    elif select_model=='SVC':    \n",
    "        # Fitting SVM to the Training set\n",
    "        classifier = SVC(kernel = 'rbf',C = 0.75, gamma = 0.2, random_state = 42).fit(X_train, y_train)\n",
    "        \n",
    "    else:\n",
    "        classifier = 'Incorrect Input'\n",
    "        \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose classifier\n",
    "\n",
    "model_selected = 'logistic'\n",
    "#model_selected = 'KNN'\n",
    "#model_selected = 'NB'\n",
    "#model_selected = 'DT'\n",
    "#model_selected = 'RF'\n",
    "#model_selected = 'SVC'\n",
    "\n",
    "classifier = classification_model(model_selected)\n",
    "print(classifier)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to make pr and roc curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# let us save TP, TN, FP, FN\n",
    "TN = cm[0,0]\n",
    "FN = cm[1,0]\n",
    "TP = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "FNR = (FN*100)/(FN+TP)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "\n",
    "print(cm)\n",
    "print('Accuracy is: %0.4f' %accuracy)\n",
    "print('FNR is: %0.3f' %FNR)\n",
    "print('Precision is: %0.4f' %precision)\n",
    "print('Recall is: %0.4f' %recall)\n",
    "print('F1 Score is: %0.4f' %f1_score)\n",
    "\n",
    "# plot Confusion Matrix\n",
    "plot_confusion_matrix(classifier, X_test, y_test)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Receiver Operating Characteristic\n",
    "# plot model roc curve \n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.', lw=2, label='area = %0.2f' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#Precision-Recall Curve\n",
    "disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
    "disp.ax_.set_title('Precision-Recall curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Test data-set and saving output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = classifier.predict(test_result)\n",
    "\n",
    "test['default'] = test_pred\n",
    "# Writing a csv (output)\n",
    "test_output.to_csv(\"test_output_python.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing output target variable\n",
    "sns.countplot(test[test.columns[-1]], alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.columns[-1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
